{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "898a6e61-383e-4b3b-9b89-5041a623b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import dlib\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f572e94-62da-4c4a-8df7-2fd2e046f946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file_path):\n",
    "    \n",
    "    # Read in image from file path\n",
    "    byte_img = tf.io.read_file(file_path)\n",
    "    # Load in the image \n",
    "    img = tf.io.decode_jpeg(byte_img)\n",
    "    \n",
    "    # Preprocessing steps - resizing the image to be 100x100x3\n",
    "    img = tf.image.resize(img, (96,96))\n",
    "    # Scale image to be between 0 and 1 \n",
    "    img = img / 255.0\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    # Return image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc97d3c0-7aea-42ff-90a5-a8ca2a2493c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using TFLIte without OPTIMIZE\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54608fc6-f743-4783-8b7f-b045721de65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "671ece93-fe23-4ee7-839e-b2e32438643d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'serving_default_conv2d_8_input:0',\n",
       "  'index': 0,\n",
       "  'shape': array([ 1, 96, 96,  3], dtype=int32),\n",
       "  'shape_signature': array([-1, 96, 96,  3], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd23280-0201-4acb-a6cd-7e4c724fef33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'StatefulPartitionedCall:0',\n",
       "  'index': 28,\n",
       "  'shape': array([1, 4], dtype=int32),\n",
       "  'shape_signature': array([-1,  4], dtype=int32),\n",
       "  'dtype': numpy.float32,\n",
       "  'quantization': (0.0, 0),\n",
       "  'quantization_parameters': {'scales': array([], dtype=float32),\n",
       "   'zero_points': array([], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6efea58-e7bc-4413-ab88-f17abb612716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_img = preprocess(os.path.join(\"dataset\", \"cropped\", \"arham\", \"9a1b3c24-99df-11ec-ab0e-842b2bb3a5b6.jpg\"))\n",
    "# input_img = preprocess(os.path.join(\"dataset\", \"cropped\", \"mohsin\", \"0a2bbaa0-99e0-11ec-826f-842b2bb3a5b6.jpg\"))\n",
    "input_img = preprocess(os.path.join(\"dataset\", \"cropped\", \"ehsan\", \"4a02d925-99e0-11ec-be8c-842b2bb3a5b6.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0424cde0-1e64-4b84-8a38-8c1232e8a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_img):\n",
    "    input_img = input_img.astype(\"float32\")\n",
    "    input_img = tf.expand_dims(input_img, 0)\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_img)\n",
    "    interpreter.invoke()\n",
    "    pred = interpreter.get_tensor(output_details[0]['index'])\n",
    "    classnames = ['arham', 'ehsan', 'mohsin', 'unknown']    \n",
    "    return classnames[np.argmax(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "255dcbe8-8613-4286-ab6a-bf5e9ed19c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Bounding Boxes\n",
    "def convert_and_trim_bb(image, rect):\n",
    "    # extract the starting and ending (x, y)-coordinates of the\n",
    "    # bounding box\n",
    "    startX = rect.left()\n",
    "    startY = rect.top()\n",
    "    endX = rect.right()\n",
    "    endY = rect.bottom()\n",
    "    # ensure the bounding box coordinates fall within the spatial\n",
    "    # dimensions of the image\n",
    "    startX = max(0, startX)\n",
    "    startY = max(0, startY)\n",
    "    endX = min(endX, image.shape[1])\n",
    "    endY = min(endY, image.shape[0])\n",
    "    # compute the width and height of the bounding box\n",
    "    w = endX - startX\n",
    "    h = endY - startY\n",
    "    # return our bounding box coordinates\n",
    "    return (startX, startY, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06de1ab4-062f-4cad-b56e-a3de0c1f1bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52dbcce0-c39a-4755-862b-4ab49685d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_faces(image):\n",
    "    \"\"\"\n",
    "    Function that gets image as input and generate face from it and returns\n",
    "    face from the image.\n",
    "    \"\"\"\n",
    "    image_height, image_width, _ = image.shape\n",
    "    faces = []\n",
    "    detections = detector(image)\n",
    "    for i, d in enumerate(detections):\n",
    "        # Croping Face from image\n",
    "        crop_img = image[max(0, d.top()): min(d.bottom(), image_height),\n",
    "                    max(0, d.left()): min(d.right(), image_width)]\n",
    "        faces.append(crop_img)\n",
    "#     returning array of faces\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "469968c0-07c4-4961-b496-e842d5b9e538",
   "metadata": {},
   "outputs": [],
   "source": [
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# org\n",
    "org = (50, 50)\n",
    "# fontScale\n",
    "fontScale = 1\n",
    "# Blue color in BGR\n",
    "color = (255, 0, 0)\n",
    "# Line thickness of 2 px\n",
    "thickness = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a558fb62-46b4-4411-9031-91caf08f1c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = cv2.VideoCapture(\"dataset/negative.mp4\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = stream.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(rgb, None, fx=1/2, fy=1/2, interpolation=cv2.INTER_AREA)\n",
    "    rects = detector(im)\n",
    "    boxes = [convert_and_trim_bb(rgb, r) for r in rects]\n",
    "    face = gen_faces(im)[0]\n",
    "    face = cv2.resize(face, (96, 96))\n",
    "    face = face / 255.\n",
    "    prediction = predict(face)\n",
    "    for (x, y, w, h) in boxes:\n",
    "        cv2.rectangle(im, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    \n",
    "    im = cv2.putText(im, prediction.capitalize(), org, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) \n",
    "    cv2.imshow(\"Feed\", im)    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "stream.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee553e-2911-4b9a-a376-af550a24e1a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
